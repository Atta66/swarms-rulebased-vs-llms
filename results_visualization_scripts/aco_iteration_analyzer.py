"""
ACO Iteration Analyzer
Analyzes ACO results with 500 iterations and shows how the average short path percentage 
changes over iterations from 0 to 500.
"""

import json
import numpy as np
import matplotlib.pyplot as plt
import os
from datetime import datetime
import glob

def find_500_iteration_aco_results():
    """Find the specific 500-iteration ACO results file from Oct 17th."""
    results_dir = "../results/aco"
    if not os.path.exists(results_dir):
        print(f"‚ùå Results directory {results_dir} not found!")
        return None
    
    # Target the specific 500-iteration files from Oct 17th (priority order)
    target_files = [
        os.path.join(results_dir, "multi_run_aco_20251017_134900.json"),
        os.path.join(results_dir, "multi_run_aco_20251017_134248.json")
    ]
    
    for target_file in target_files:
        if os.path.exists(target_file):
            print(f"üìä Using 500-iteration ACO results: {os.path.basename(target_file)}")
            return target_file
    
    # If no 500-iteration files found, search for any with 500 iterations
    print("üîç Searching for any ACO results with 500 iterations...")
    pattern = os.path.join(results_dir, "multi_run_aco_*.json")
    files = glob.glob(pattern)
    
    for file_path in files:
        try:
            with open(file_path, 'r') as f:
                # Read just enough to find configuration
                content = f.read(10000)  # Read first 10KB to find config
                if '"max_iterations": 500' in content or '"max_iterations":500' in content:
                    print(f"‚úÖ Found 500-iteration ACO results: {os.path.basename(file_path)}")
                    return file_path
        except:
            continue
    
    print("‚ùå No 500-iteration ACO results found!")
    return None

def load_and_analyze_results(file_path):
    """Load ACO results and analyze iteration-by-iteration progression."""
    try:
        with open(file_path, 'r') as f:
            data = json.load(f)
        
        print(f"‚úÖ Loaded results file")
        
        # Check the actual structure of the data
        print(f"üìã Available keys: {list(data.keys())}")
        
        # Extract configuration info
        config = data.get('configuration', {})
        max_iterations = config.get('max_iterations', 'Unknown')
        
        # Verify this is a 500-iteration file
        if max_iterations != 500:
            print(f"‚ö†Ô∏è  Warning: File has {max_iterations} iterations, not 500!")
        
        # Look for individual results in different possible locations
        individual_results = None
        for key in ['individual_results', 'results', 'trials', 'data']:
            if key in data:
                individual_results = data[key]
                print(f"üìä Found individual results in '{key}' with {len(individual_results)} trials")
                break
        
        if individual_results is None:
            # Check if the data itself is a list
            if isinstance(data, list):
                individual_results = data
                print(f"üìä Data is directly a list with {len(individual_results)} trials")
            else:
                print(f"‚ùå Could not find individual results in the data structure")
                print(f"Available top-level keys: {list(data.keys())}")
                return None, None
        
        num_trials = len(individual_results)
        print(f"üìã Configuration: {max_iterations} iterations, {num_trials} trials")
        
        # Analyze step-by-step progression
        all_iterations_data = []
        
        for trial_idx, trial in enumerate(individual_results):
            # Look for detailed step data in different possible locations
            trial_data = None
            for key in ['detailed_step_data', 'step_data', 'steps', 'iterations']:
                if key in trial:
                    trial_data = trial[key]
                    break
            
            if trial_data is None:
                print(f"‚ö†Ô∏è  Trial {trial_idx + 1}: No step data found")
                if trial_idx == 0:  # Show structure of first trial
                    print(f"   Available keys in trial: {list(trial.keys())}")
                continue
            
            for step_idx, step in enumerate(trial_data):
                iteration = step_idx + 1  # 1-based iteration numbering
                
                # Calculate short path percentage for this step
                # Look for the path choice information
                chosen = step.get('chosen', None)
                
                if chosen is not None:
                    # Count this selection
                    if chosen == "short":
                        short_percentage = 100.0  # This iteration chose short
                    else:
                        short_percentage = 0.0    # This iteration chose long
                else:
                    # Try alternative data structures
                    short_selections = step.get('short_selections', 0)
                    total_selections = step.get('total_ants', step.get('total_selections', 1))
                    
                    if total_selections > 0:
                        short_percentage = (short_selections / total_selections) * 100
                    else:
                        short_percentage = 0
                
                all_iterations_data.append({
                    'trial': trial_idx + 1,
                    'iteration': iteration,
                    'short_percentage': short_percentage,
                    'chosen': chosen
                })
        
        if not all_iterations_data:
            print("‚ùå No iteration data could be extracted")
            return None, None
            
        print(f"üìä Successfully extracted {len(all_iterations_data)} data points")
        return all_iterations_data, config
        
    except Exception as e:
        print(f"‚ùå Error loading results: {e}")
        import traceback
        traceback.print_exc()
        return None, None

def calculate_iteration_averages(iterations_data, max_iterations):
    """Calculate average short percentage for each iteration across all trials."""
    iteration_averages = {}
    
    # Group data by iteration
    for data_point in iterations_data:
        iteration = data_point['iteration']
        short_pct = data_point['short_percentage']
        
        if iteration not in iteration_averages:
            iteration_averages[iteration] = []
        
        iteration_averages[iteration].append(short_pct)
    
    # Calculate averages and standard deviations
    iterations = []
    averages = []
    std_devs = []
    
    # Only process iterations that we have data for
    available_iterations = sorted(iteration_averages.keys())
    max_available = max(available_iterations) if available_iterations else 0
    
    print(f"üìä Available iterations: 1 to {max_available}")
    
    for iteration in range(1, min(max_iterations, max_available) + 1):
        if iteration in iteration_averages:
            values = iteration_averages[iteration]
            avg = np.mean(values)
            std = np.std(values)
            
            iterations.append(iteration)
            averages.append(avg)
            std_devs.append(std)
    
    print(f"üìà Processed {len(iterations)} iterations with data")
    return iterations, averages, std_devs

def smooth_data(iterations, averages, std_devs, window_size=10):
    """Apply moving average to smooth the data for better readability."""
    if len(averages) == 0:
        return iterations, averages, std_devs
    
    # Apply moving average
    smoothed_averages = []
    smoothed_std_devs = []
    smoothed_iterations = []
    
    for i in range(len(averages)):
        # Define window boundaries
        start_idx = max(0, i - window_size // 2)
        end_idx = min(len(averages), i + window_size // 2 + 1)
        
        # Calculate moving average
        window_avgs = averages[start_idx:end_idx]
        window_stds = std_devs[start_idx:end_idx]
        
        smoothed_avg = np.mean(window_avgs)
        smoothed_std = np.mean(window_stds)  # Average the standard deviations
        
        smoothed_averages.append(smoothed_avg)
        smoothed_std_devs.append(smoothed_std)
        smoothed_iterations.append(iterations[i])
    
    return smoothed_iterations, smoothed_averages, smoothed_std_devs

def create_progression_graph(iterations, averages, std_devs, config):
    """Create a graph showing the progression of short path percentage over iterations."""
    
    # Convert to numpy arrays for easier manipulation
    iterations = np.array(iterations)
    averages = np.array(averages)
    std_devs = np.array(std_devs)
    
    # Apply smoothing for better readability
    smooth_iter, smooth_avg, smooth_std = smooth_data(iterations, averages, std_devs, window_size=20)
    smooth_iter = np.array(smooth_iter)
    smooth_avg = np.array(smooth_avg)
    smooth_std = np.array(smooth_std)
    
    # Create single plot
    plt.figure(figsize=(16, 8))
    
    # Main plot with only smoothed data
    plt.plot(smooth_iter, smooth_avg, 'darkblue', linewidth=3, label='Average Optimal Path Selection')
    
    # Add confidence interval for smoothed data (clamp to 0-100% range)
    lower_bound = np.maximum(smooth_avg - smooth_std, 0)  # Don't go below 0%
    upper_bound = np.minimum(smooth_avg + smooth_std, 100)  # Don't go above 100%
    
    plt.fill_between(smooth_iter, 
                     lower_bound, 
                     upper_bound, 
                     alpha=0.2, color='darkblue', label='¬±1 Standard Deviation (Trial Variability)')
    
    # Formatting
    plt.xlabel('Iteration', fontsize=14)
    plt.ylabel('Optimal Path Selection (%)', fontsize=14)
    
    plt.grid(True, alpha=0.3)
    plt.legend(fontsize=12)
    plt.ylim(0, 100)
    
    # Add some key statistics as text
    final_avg = smooth_avg[-1] if len(smooth_avg) > 0 else 0
    initial_avg = smooth_avg[0] if len(smooth_avg) > 0 else 0
    max_avg = np.max(smooth_avg) if len(smooth_avg) > 0 else 0
    
    stats_text = f"Initial: {initial_avg:.1f}%\nFinal: {final_avg:.1f}%\nPeak: {max_avg:.1f}%"
    plt.text(0.02, 0.98, stats_text, transform=plt.gca().transAxes, 
             verticalalignment='top', bbox=dict(boxstyle='round', facecolor='wheat', alpha=0.8),
             fontsize=12)
    
    plt.tight_layout()
    
    # Save the plot
    timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
    filename = f"aco_500_iteration_progression_{timestamp}.png"
    save_path = os.path.join("..", "results", "visualizations", filename)
    
    # Ensure directory exists
    os.makedirs(os.path.dirname(save_path), exist_ok=True)
    
    plt.savefig(save_path, dpi=300, bbox_inches='tight')
    print(f"üíæ 500-iteration graph saved to: {save_path}")
    
    plt.show()
    
    return save_path

def print_phase_analysis(iterations, averages, max_iterations):
    """Print analysis of different phases of learning."""
    if len(averages) == 0:
        return
    
    # Define phases for 500 iterations
    early_end = max_iterations // 3  # First ~167 iterations
    mid_end = 2 * max_iterations // 3  # Middle ~167-333 iterations
    
    early_phase = [avg for i, avg in enumerate(averages) if iterations[i] <= early_end]
    mid_phase = [avg for i, avg in enumerate(averages) if early_end < iterations[i] <= mid_end]
    late_phase = [avg for i, avg in enumerate(averages) if iterations[i] > mid_end]
    
    print("\n" + "="*60)
    print("üìä PHASE ANALYSIS (500 ITERATIONS)")
    print("="*60)
    
    if early_phase:
        print(f"üîç Early Phase (1-{early_end}): {np.mean(early_phase):.1f}% ¬± {np.std(early_phase):.1f}%")
    
    if mid_phase:
        print(f"üéØ Mid Phase ({early_end+1}-{mid_end}): {np.mean(mid_phase):.1f}% ¬± {np.std(mid_phase):.1f}%")
    
    if late_phase:
        print(f"üöÄ Late Phase ({mid_end+1}-{max_iterations}): {np.mean(late_phase):.1f}% ¬± {np.std(late_phase):.1f}%")
    
    # Learning trend
    if len(averages) >= 2:
        learning_rate = (averages[-1] - averages[0]) / len(averages)
        print(f"üìà Learning Rate: {learning_rate:.3f}% per iteration")
    
    # Additional 500-iteration specific analysis
    if len(averages) >= 500:
        convergence_80 = None
        convergence_90 = None
        convergence_95 = None
        
        for i, avg in enumerate(averages):
            if convergence_80 is None and avg >= 80:
                convergence_80 = iterations[i]
            if convergence_90 is None and avg >= 90:
                convergence_90 = iterations[i]
            if convergence_95 is None and avg >= 95:
                convergence_95 = iterations[i]
        
        print(f"\nüéØ Convergence Analysis:")
        if convergence_80:
            print(f"   80% accuracy reached at iteration: {convergence_80}")
        if convergence_90:
            print(f"   90% accuracy reached at iteration: {convergence_90}")
        if convergence_95:
            print(f"   95% accuracy reached at iteration: {convergence_95}")

def main():
    """Main analysis function."""
    print("üêú ACO 500-Iteration Analysis")
    print("="*50)
    
    # Find and load the 500-iteration results
    target_file = find_500_iteration_aco_results()
    if not target_file:
        return
    
    # Load and analyze the data
    iterations_data, config = load_and_analyze_results(target_file)
    if not iterations_data:
        return
    
    max_iterations = config.get('max_iterations', 500)
    print(f"üìä Analyzing {len(iterations_data)} data points across {max_iterations} iterations")
    
    # Calculate iteration averages
    iterations, averages, std_devs = calculate_iteration_averages(iterations_data, max_iterations)
    
    # Create the progression graph with smoothing
    save_path = create_progression_graph(iterations, averages, std_devs, config)
    
    # Print phase analysis
    print_phase_analysis(iterations, averages, max_iterations)
    
    print(f"\n‚úÖ 500-iteration analysis complete! Graph saved to: {save_path}")

if __name__ == "__main__":
    main()
